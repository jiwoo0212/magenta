{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music\n",
    "\n",
    "# Abstract\n",
    "VAE는 풍부한 의미를 담는 latent representation을 학습하는데에 좋은 모델이다. 그러나 latent code를 무시하고 생성을 하는 'posterior collapse' 문제가 존재한다. 이러한 기존 VAE의 문제점을 해결하고자 hierarchical decoder를 사용하여 모델이 latent code를 활용하도록 유도한다.\n",
    "\n",
    "\n",
    "# 기존의 Recurrent VAE\n",
    "<img src=\"https://ars.els-cdn.com/content/image/1-s2.0-S0020025519302786-gr3.jpg\" width= 600 heigth=300>\n",
    "\n",
    "## Encoder\n",
    "인코더는 rnn cell로 이뤄져있다. 사후확률 $q_\\lambda(z \\mid x)$를 $p(z)$ 에 근사하도록 학습한다. \n",
    "인코딩의 결과물로 latent 분포를 나타내는 $\\mu$ $\\sigma$ 를 도출한다.\n",
    "\n",
    "## Decoder\n",
    "디코더는 rnn cell로 이뤄져있다. 인코더의 $z$를 받아서 output을 생성한다.\n",
    "\n",
    "## 문제점\n",
    "- latent code를 무시하는 경우가 있다.\n",
    "- 하나의 latent vector에 전체 시퀀스 정보를 압축하기에, 짧은 시퀀스에서만 잘 작동하는 경우가 있다.\n",
    "---\n",
    "\n",
    "# MusicVAE Model\n",
    "<img src=\"https://velog.velcdn.com/images/ann9902/post/c47ee5a6-c06d-4663-b2b0-8a3ac15eb222/image.png\" width=600 heigth=300>\n",
    "\n",
    "## Bidrectional Encoder\n",
    "two-layer bidirectional LSTM을 사용한다. 양방향의 hidden state, $\\vec{h}_T, \\overleftarrow{h}_T$를 concat하여 최종적인 hidden state를 만들고, 이를 fullyconnected layer를 이용해 $\\mu$와 $\\sigma$ 를 만든다. 해당 과정의 수식은 아래와 같다. \n",
    "\n",
    "$\\begin{aligned} & \\mu=W_{h \\mu} h_T+b_\\mu \\\\ & \\sigma=\\log \\left(\\exp \\left(W_{h \\sigma} h_T+b_\\sigma\\right)+1\\right)\\end{aligned}$\n",
    "\n",
    "LSTM의 hidden state size는 2048이고, latent dimension은 512를 가진다. bidirectional 인코더를 사용하기에 long-term 시퀀스를 parameterize하기에 용이하다. \n",
    "\n",
    "<img src=\"https://velog.velcdn.com/images/ann9902/post/f7f0fd9f-2ff3-4b46-bbe0-cda86e425ccc/image.png\" width=600 heigth=300>\n",
    "\n",
    "## Hierarchical Decoder\n",
    "기존의 recurrent VAE는 인코더의 $Z$를 디코더의 initial state로 해서 output을 autoregressive하게 생성한다. 이는 long sequence를 잘 생성하지 못한다. 이는 latent code의 영향력이 뒤로 갈수록 손실되기 때문이라고 짐작된다. 이러한 문제를 해결하기 위해 hierarchical한 구조를 도입했다. \n",
    "\n",
    "생성하려고 하는 시퀀스가 u개의 subsequence로 나눠져있다고 가정하고, \n",
    "우선 z를 fully-connected layer와 tanh activation를 통과시켜 “conductor” RNN의 initial state를 얻는다. 그리고 conductor를 이용해서 u개의 임베딩(각 subsequence를 위한 것)을 만든다. \n",
    "\n",
    "만들어진 임베딩을 fully-connected layer와 tanh activation를 통과시켜 디코더의 마지막 RNN(이하 decoder RNN)의 initial state를 만든다. decoder RNN이 autoregressive하게 output을 생성하고 이를 softmax에 통과시켜 최종 output token을 예측한다. 이전의 output token과 현재의 conductor의 임베딩을 concat해서 현재 decoder RNN에 입력한다. \n",
    "\n",
    "앞서 설명한 모델도 여전히 “posterior collapse 문제가 발생할 수 있지만, 디코더의 스코프를 줄여서 latent code를 쓰도록 강제하는 것이 중요하다는 것을 발견했다. 이러한 제한은 CNN decoder에서는 receptive field를 줄여서 쉽게 가능하지만 RNN에서는 정해진 방법이 없다. 그렇기에 각 서브시퀀스의 RNN state를 conducter의 임베딩으로 initialize하여, 디코더가 long term한 정보를 얻기 위해서 latent code로부터 만들어진 conductor의 임베딩을 활용할 수밖에 없도록하는 방법을 채택했다.\n",
    "\n",
    "<img src=\"https://velog.velcdn.com/images/ann9902/post/9e344f16-f875-412e-a332-9a8001218d3c/image.png\" width=600 heigth=300>\n",
    "\n",
    "## Reconstruction Quality\n",
    "hierarchical한 구조를 사용하지 않을때보다 reconstruction accuracy가 높은 것을 볼 수 있다.\n",
    "\n",
    "<img src=\"https://velog.velcdn.com/images/ann9902/post/03115e5d-62fa-4b15-bd4a-ce6eaf12aa5e/image.png\" width=600 heigth=300>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 구현된 코드 살펴보기 \n",
    "\n",
    "## 처리 순서\n",
    "1. midi → notesequence → tfRecord → 모델 input형태 순으로 데이터 처리를 한다. \n",
    "2. train\n",
    "3. generate\n",
    "\n",
    "각 단계에서 아래의 모듈을 사용한다.\n",
    "\n",
    "midi → notesequence : `magenta\\scripts\\convert_dir_to_note_sequences.py`\n",
    "\n",
    "tfRecord → model input  : `magenta\\models\\music_vae\\preprocess_tfrecord.py`\n",
    "\n",
    "train : `magenta\\models\\music_vae\\music_vae_train.py`\n",
    "\n",
    "generate : `magenta\\models\\music_vae\\music_vae_generate.py`\n",
    "\n",
    "\n",
    "1_midi_to_tf.sh > 2_tf_to_input.sh > 3_train.sh > 4_generate.sh 순서로 실행하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래에서 모듈의 주요부분을 살펴보자.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리\n",
    "\n",
    "## midi -> notesequence(tfrecord)\n",
    "\n",
    "scripts/convert_dir_to_note_sequences.py > def convert_midi, def convert_directory\n",
    "\n",
    "midi_to_sequence_proto로 midi를 notesequence로 만들고, TFRecordWriter로 이를 tfrecord로 저장한다.\n",
    "\n",
    "```python\n",
    "def convert_midi(root_dir, sub_dir, full_file_path):\n",
    "  \"\"\"Converts a midi file to a sequence proto.\n",
    "\n",
    "  Args:\n",
    "    root_dir: A string specifying the root directory for the files being\n",
    "        converted.\n",
    "    sub_dir: The directory being converted currently.\n",
    "    full_file_path: the full path to the file to convert.\n",
    "\n",
    "  Returns:\n",
    "    Either a NoteSequence proto or None if the file could not be converted.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    sequence = midi_io.midi_to_sequence_proto( # notesequence로 만드는 부분\n",
    "        tf.gfile.GFile(full_file_path, 'rb').read())\n",
    "  except midi_io.MIDIConversionError as e:\n",
    "    tf.logging.warning(\n",
    "        'Could not parse MIDI file %s. It will be skipped. Error was: %s',\n",
    "        full_file_path, e)\n",
    "    return None\n",
    "  sequence.collection_name = os.path.basename(root_dir)\n",
    "  sequence.filename = os.path.join(sub_dir, os.path.basename(full_file_path))\n",
    "  sequence.id = generate_note_sequence_id(\n",
    "      sequence.filename, sequence.collection_name, 'midi') \n",
    "  tf.logging.info('Converted MIDI file %s.', full_file_path)\n",
    "  return sequence\n",
    "```\n",
    "\n",
    "```python\n",
    "def convert_directory(root_dir, output_file, recursive=False):\n",
    "  \"\"\"Converts files to NoteSequences and writes to `output_file`.\n",
    "\n",
    "  Input files found in `root_dir` are converted to NoteSequence protos with the\n",
    "  basename of `root_dir` as the collection_name, and the relative path to the\n",
    "  file from `root_dir` as the filename. If `recursive` is true, recursively\n",
    "  converts any subdirectories of the specified directory.\n",
    "\n",
    "  Args:\n",
    "    root_dir: A string specifying a root directory.\n",
    "    output_file: Path to TFRecord file to write results to.\n",
    "    recursive: A boolean specifying whether or not recursively convert files\n",
    "        contained in subdirectories of the specified directory.\n",
    "  \"\"\"\n",
    "  with tf.io.TFRecordWriter(output_file) as writer:\n",
    "    convert_files(root_dir, '', writer, recursive)\n",
    "\n",
    "```\n",
    "## make model input\n",
    "\n",
    "music_vae/preprocess_tfrecord.py \n",
    "\n",
    "DrumsConverter를 적용하여 notesequence를 model input으로 만든다. \n",
    "\n",
    "```python\n",
    "class DrumsConverter(BaseNoteSequenceConverter):\n",
    "  \"\"\"Converter for legacy drums with either pianoroll or one-hot tensors.\n",
    "\n",
    "  Inputs/outputs are either a \"pianoroll\"-like encoding of all possible drum\n",
    "  hits at a given step, or a one-hot encoding of the pianoroll.\n",
    "\n",
    "  The \"roll\" input encoding includes a final NOR bit (after the optional end\n",
    "  token).\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 구조"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "CONFIG_MAP['hierdec-drum_4bar'] = Config(\n",
    "    model=MusicVAE(\n",
    "        lstm_models.BidirectionalLstmEncoder(),\n",
    "        lstm_models.HierarchicalLstmDecoder(\n",
    "            lstm_models.CategoricalLstmDecoder(),\n",
    "            level_lengths=[8, 8],\n",
    "            disable_autoregression=True)),\n",
    "    hparams=merge_hparams(\n",
    "        lstm_models.get_default_hparams(),\n",
    "        HParams(\n",
    "            batch_size=64,\n",
    "            max_seq_len=64,  # 16*4\n",
    "            z_size=512,\n",
    "            enc_rnn_size=[2048, 2048],\n",
    "            dec_rnn_size=[1024, 1024],\n",
    "            free_bits=256,\n",
    "            max_beta=0.2,\n",
    "        )),\n",
    "    note_sequence_augmenter=None,\n",
    "    data_converter=data.DrumsConverter(\n",
    "        max_bars=100,  # Truncate long drum sequences before slicing.\n",
    "        slice_bars=4,  # 4 마디\n",
    "        steps_per_quarter=4,\n",
    "        roll_input=True),\n",
    "    train_examples_path=None,\n",
    "    eval_examples_path=None,\n",
    ")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MusicVAE안에 인코더로 BidirectionalLstmEncoder, 디코더로 HierarchicalLstmDecoder, CategoricalLstmDecoder가 들어있다.\n",
    "\n",
    "논문에서 conductor RNN가 HierarchicalLstmDecoder에 해당하는 것이고, decoder RNN이 CategoricalLstmDecoder이다.\n",
    "\n",
    "max_seq_length인자를 64로 설정하면 16*4, 4마디를 생성할 수 있다. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인코더\n",
    "### Bidirectional Encoder\n",
    "\n",
    "lstm_utils.py > def build_bidirectional_lstm\n",
    "\n",
    "양방향 lstm을 만든다.\n",
    "\n",
    "```python\n",
    "def build_bidirectional_lstm(\n",
    "    layer_sizes, dropout_keep_prob, residual, is_training):\n",
    "  \"\"\"Build the Tensorflow graph for a bidirectional LSTM.\"\"\"\n",
    "\n",
    "  cells_fw = []\n",
    "  cells_bw = []\n",
    "  for layer_size in layer_sizes:\n",
    "    cells_fw.append(\n",
    "        rnn_cell([layer_size], dropout_keep_prob, residual, is_training))\n",
    "    cells_bw.append(\n",
    "        rnn_cell([layer_size], dropout_keep_prob, residual, is_training))\n",
    "\n",
    "  return cells_fw, cells_bw\n",
    "```\n",
    "---\n",
    "\n",
    "lstm_models.py > BidirectionalLstmEncoder클래스 > def encode\n",
    "\n",
    "양방향의 RNN 결과물을 concat하고 있다.\n",
    "\n",
    "```python\n",
    "  def encode(self, sequence, sequence_length):\n",
    "    cells_fw, cells_bw = self._cells\n",
    "\n",
    "    _, states_fw, states_bw = contrib_rnn.stack_bidirectional_dynamic_rnn(\n",
    "        cells_fw,\n",
    "        cells_bw,\n",
    "        sequence,\n",
    "        sequence_length=sequence_length,\n",
    "        time_major=False,\n",
    "        dtype=tf.float32,\n",
    "        scope=self._name_or_scope)\n",
    "    # Note we access the outputs (h) from the states since the backward\n",
    "    # ouputs are reversed to the input order in the returned outputs.\n",
    "    last_h_fw = states_fw[-1][-1].h\n",
    "    last_h_bw = states_bw[-1][-1].h\n",
    "\n",
    "    return tf.concat([last_h_fw, last_h_bw], 1)\n",
    "```\n",
    "---\n",
    "\n",
    "base_models.py > class MusicVAE > def encode\n",
    "\n",
    "앞서 만든 양방향 rnn 결과물로 $\\mu$, $\\sigma$ 를 만들고 최종적으로 latent code를 반환한다.\n",
    "\n",
    "```python\n",
    "  def encode(self, sequence, sequence_length, control_sequence=None):\n",
    "    \"\"\"Encodes input sequences into a MultivariateNormalDiag distribution.\n",
    "\n",
    "    Args:\n",
    "      sequence: A Tensor with shape `[num_sequences, max_length, input_depth]`\n",
    "          containing the sequences to encode.\n",
    "      sequence_length: The length of each sequence in the `sequence` Tensor.\n",
    "      control_sequence: (Optional) A Tensor with shape\n",
    "          `[num_sequences, max_length, control_depth]` containing control\n",
    "          sequences on which to condition. These will be concatenated depthwise\n",
    "          to the input sequences.\n",
    "\n",
    "    Returns:\n",
    "      A tfp.distributions.MultivariateNormalDiag representing the posterior\n",
    "      distribution for each sequence.\n",
    "    \"\"\"\n",
    "    hparams = self.hparams\n",
    "    z_size = hparams.z_size\n",
    "\n",
    "    sequence = tf.to_float(sequence)\n",
    "    if control_sequence is not None:\n",
    "      control_sequence = tf.to_float(control_sequence)\n",
    "      sequence = tf.concat([sequence, control_sequence], axis=-1)\n",
    "    encoder_output = self.encoder.encode(sequence, sequence_length) # 앞서 설명한 concat한 결과물\n",
    "\n",
    "    mu = tf.layers.dense(\n",
    "        encoder_output,\n",
    "        z_size,\n",
    "        name='encoder/mu',\n",
    "        kernel_initializer=tf.random_normal_initializer(stddev=0.001))\n",
    "    sigma = tf.layers.dense(\n",
    "        encoder_output,\n",
    "        z_size,\n",
    "        activation=tf.nn.softplus,\n",
    "        name='encoder/sigma',\n",
    "        kernel_initializer=tf.random_normal_initializer(stddev=0.001))\n",
    "\n",
    "    return ds.MultivariateNormalDiag(loc=mu, scale_diag=sigma) # latent code\n",
    "``` "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical decoder\n",
    "\n",
    "## conductor RNN\n",
    "\n",
    "lstm_models.py > class HierarchicalLstmDecoder > def _hierarchical_decode\n",
    "\n",
    "conductor RNN에서 인코더에서 넘어온 z를 받아 임베딩을 만든다.\n",
    "\n",
    "```python\n",
    "  def _hierarchical_decode(self, z, base_decode_fn):\n",
    "    \"\"\"Depth first decoding from `z`, passing final embeddings to base fn.\"\"\"\n",
    "    batch_size = z.shape[0]\n",
    "    # Subtract 1 for the core decoder level.\n",
    "    num_levels = len(self._level_lengths) - 1\n",
    "\n",
    "    hparams = self.hparams\n",
    "    batch_size = hparams.batch_size\n",
    "\n",
    "    def recursive_decode(initial_input, path=None):\n",
    "      \"\"\"Recursive hierarchical decode function.\"\"\"\n",
    "      path = path or []\n",
    "      level = len(path)\n",
    "\n",
    "      if level == num_levels:\n",
    "        with tf.variable_scope('core_decoder', reuse=tf.AUTO_REUSE):\n",
    "          return base_decode_fn(initial_input, path)\n",
    "\n",
    "      scope = tf.VariableScope(\n",
    "          tf.AUTO_REUSE, 'decoder/hierarchical_level_%d' % level)\n",
    "      num_steps = self._level_lengths[level]\n",
    "      with tf.variable_scope(scope):\n",
    "        state = lstm_utils.initial_cell_state_from_embedding( \n",
    "            self._hier_cells[level], initial_input, name='initial_state')\n",
    "      if level not in self._disable_autoregression:\n",
    "        # The initial input should be the same size as the tensors returned by\n",
    "        # next level.\n",
    "        if self._hierarchical_encoder:\n",
    "          input_size = self._hierarchical_encoder.level(0).output_depth\n",
    "        elif level == num_levels - 1:\n",
    "          input_size = sum(tf.nest.flatten(self._core_decoder.state_size))\n",
    "        else:\n",
    "          input_size = sum(\n",
    "              tf.nest.flatten(self._hier_cells[level + 1].state_size))\n",
    "        next_input = tf.zeros([batch_size, input_size])\n",
    "      lower_level_embeddings = []\n",
    "      for i in range(num_steps):\n",
    "        if level in self._disable_autoregression:\n",
    "          next_input = tf.zeros([batch_size, 1])\n",
    "        else:\n",
    "          next_input = tf.concat([next_input, initial_input], axis=1)\n",
    "        with tf.variable_scope(scope):\n",
    "          output, state = self._hier_cells[level](next_input, state, scope)\n",
    "        next_input = recursive_decode(output, path + [i])\n",
    "        lower_level_embeddings.append(next_input) \n",
    "      if self._hierarchical_encoder:\n",
    "        # Return the encoding of the outputs using the appropriate level of the\n",
    "        # hierarchical encoder.\n",
    "        enc_level = num_levels - level\n",
    "        return self._hierarchical_encoder.level(enc_level).encode(\n",
    "            sequence=tf.stack(lower_level_embeddings, axis=1),\n",
    "            sequence_length=tf.fill([batch_size], num_steps))\n",
    "      else:\n",
    "        # Return the final state.\n",
    "        return tf.concat(tf.nest.flatten(state), axis=-1)\n",
    "\n",
    "    return recursive_decode(z)\n",
    "```\n",
    "---\n",
    "\n",
    "## decoder RNN\n",
    "\n",
    "lstm_models.CategoricalLstmDecoder()에 해당한다. \n",
    "\n",
    "lstm_models.py > class BaseLstmDecoder > def _decode\n",
    "\n",
    "conductor의 임베딩을 받아 decoder RNN의 initial state를 만든다.\n",
    "\n",
    "```python\n",
    "  def _decode(self, z, helper, input_shape, max_length=None):\n",
    "    \"\"\"Decodes the given batch of latent vectors vectors, which may be 0-length.\n",
    "\n",
    "    Args:\n",
    "      z: Batch of latent vectors, sized `[batch_size, z_size]`, where `z_size`\n",
    "        may be 0 for unconditioned decoding.\n",
    "      helper: A seq2seq.Helper to use.\n",
    "      input_shape: The shape of each model input vector passed to the decoder.\n",
    "      max_length: (Optional) The maximum iterations to decode.\n",
    "\n",
    "    Returns:\n",
    "      results: The LstmDecodeResults.\n",
    "    \"\"\"\n",
    "    initial_state = lstm_utils.initial_cell_state_from_embedding( # 임베딩으로부터 inintial state를 만들고 있다.\n",
    "        self._dec_cell, z, name='decoder/z_to_initial_state')\n",
    "\n",
    "    decoder = lstm_utils.Seq2SeqLstmDecoder(\n",
    "        self._dec_cell,\n",
    "        helper,\n",
    "        initial_state=initial_state,\n",
    "        input_shape=input_shape,\n",
    "        output_layer=self._output_layer)\n",
    "    final_output, final_state, final_lengths = contrib_seq2seq.dynamic_decode(\n",
    "        decoder,\n",
    "        maximum_iterations=max_length,\n",
    "        swap_memory=True,\n",
    "        scope='decoder')\n",
    "    results = lstm_utils.LstmDecodeResults(\n",
    "        rnn_input=final_output.rnn_input[:, :, :self._output_depth],\n",
    "        rnn_output=final_output.rnn_output,\n",
    "        samples=final_output.sample_id,\n",
    "        final_state=final_state,\n",
    "        final_sequence_lengths=final_lengths)\n",
    "\n",
    "    return results\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4aaf93a0998bd11bf79cd168406af597568ecf9c8276104ffaf7642a88ce49c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
